{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03176981-8e4e-4f7a-9212-e7de92731acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hoss\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import hoss.tools.upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877dcd2-1bfa-49b2-9bbd-6f08ea14cf89",
   "metadata": {},
   "source": [
    "## Connect to local server\n",
    "This notebook demonstrates how to use the upload tool that is included in the hoss client library.\n",
    "\n",
    "For these demo notebooks, it's assumed you're running against the system running in\n",
    "dev mode and able to connect to localhost.\n",
    "\n",
    "We start by connecting the the \"local\" server. If using a different server be sure to change the `.connect()` arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42957e48-f1c5-4511-8dd9-a5f7b02e78fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_local = hoss.connect('http://localhost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5450b39-bf83-4bd9-ac19-f2b85c5010fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Existing Namespaces:\")\n",
    "print(server_local.list_namespaces())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730df799-88a8-4ea3-9fd3-09b19f6387e1",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "First load the default namespace and then create a dataset inside the namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978bae0-533f-4c56-a3f7-e4ea56024545",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = server_local.get_namespace('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5aad2e-80a2-480b-b7a5-a7a32857270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ns.create_dataset(\"upload-test\", \"A dataset for an upload tool example\")\n",
    "ds.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc064fb-8633-4bb1-b89c-b99a02d30d4f",
   "metadata": {},
   "source": [
    "## Write test data to upload\n",
    "\n",
    "The upload tool operates on a directory of files. Create a test directory of dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f5187-8bfd-4041-867f-33219b72c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "for cnt in range(5):\n",
    "    with open(os.path.join(temp_dir.name, f\"file{cnt}.dat\"), 'wt') as fh:\n",
    "        fh.write('dummy data' * 5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e483e-e965-497b-89c8-a3f7c29ef78f",
   "metadata": {},
   "source": [
    "## Run upload tool\n",
    "\n",
    "You can run the upload tool as a function that even works in Jupyter.\n",
    "\n",
    "You can also run the upload tool from the command line. When you pip install the hoss client library, the program `hoss` is installed. The format of the command line interface is:\n",
    "\n",
    "`hoss upload <dataset name> <absolute path to the upload dir>`\n",
    "\n",
    "You can optionally write metadata key-value pairs using the `-m` flag (i.e `-m subject_id=123`). Multiple `-m` optional args are supported.\n",
    "\n",
    "You can optionally filter out files to upload using a regex string with the `--skip` arg.\n",
    "\n",
    "You can specify the endpoint (defaults to localhost) using the `--endpoint` arg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e04de4-5fa7-4d35-a672-894d5366d37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hoss upload [OPTIONS] DATASET_NAME DIRECTORY\n",
      "\n",
      "  Upload files in a directory to an existing dataset\n",
      "\n",
      "Options:\n",
      "  -n, --namespace TEXT            Namespace that contains the dataset\n",
      "                                  [default: default]\n",
      "  -e, --endpoint TEXT             Hoss server root endpoint  [default:\n",
      "                                  http://localhost]\n",
      "  -p, --prefix TEXT               Optional prefix to where the files should be\n",
      "                                  uploaded. If this is not provided, the files\n",
      "                                  will be uploaded to a 'directory' in the\n",
      "                                  root of the dataset. The the 'directory'\n",
      "                                  name will be the same as the source\n",
      "                                  directory name.\n",
      "  -s, --skip TEXT                 Optional regular expression used to filter\n",
      "                                  out files to skip (e.g. myprefix.*\\.txt)\n",
      "  -j, --num_processes INTEGER     Number of processes to use when uploading\n",
      "                                  files. If you have too many processes you'll\n",
      "                                  run out of bandwidth and uploads will\n",
      "                                  timeout/fail. If you don't have enough, your\n",
      "                                  upload could take more time. In general, if\n",
      "                                  you have lots of small files you benefit\n",
      "                                  from more processes, and if you have large\n",
      "                                  files, you likely don't need that many\n",
      "                                  because boto will use concurrent uploads\n",
      "                                  [default: 1]\n",
      "  -c, --max_concurrency INTEGER   Maximum number of concurrent s3 API transfer\n",
      "                                  operations.  [default: 10]\n",
      "  --multipart_threshold INTEGER   Threshold in megabytes for which transfers\n",
      "                                  will be split into multiple parts, defaults\n",
      "                                  to 32MB  [default: 32]\n",
      "  --multipart_chunk_size INTEGER  Size in megabytes for each multipart chunk,\n",
      "                                  if used. Defaults to 32MB  [default: 32]\n",
      "  -m, --metadata TEXT             Object metadata key-value pair(s) applied to\n",
      "                                  every object uploaded. You may specify\n",
      "                                  multiple values by repeating the option\n",
      "                                  (e.g. -m foo=bar -m fizz=buzz\n",
      "  -h, --help                      Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!hoss upload -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf8754-6671-4503-b6b6-12edeaed5df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try uploading by using the function directly\n",
    "# We can populate most args using the client library objects we've already created\n",
    "hoss.tools.upload.upload_directory(ds.dataset_name, temp_dir.name, ns.name, server_local.base_url, num_processes=1,\n",
    "                                   skip=None, max_concurrency=10, multipart_threshold=48, multipart_chunk_size=48, metadata={\"my-upload-test\": \"foo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32bc22-ba2f-4f1e-9ba5-09c236bf30f2",
   "metadata": {},
   "source": [
    "## Verify the files uploaded successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea4c51-6e11-428d-ac65-e4e2a4baf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in (ds / \"my-test\").iterdir():\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f57542-c3e5-4fd3-940d-a7dca9c6ec59",
   "metadata": {},
   "source": [
    "## Clean up this example\n",
    "Run these cells to remove the resources created during the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2283d88-2293-4c18-8b52-3bc6be014f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe09148-28bb-4b3f-8e22-83f0c489ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.delete_dataset(\"upload-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2119697-2dfe-4d94-a3cf-3816e7ddbd67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
